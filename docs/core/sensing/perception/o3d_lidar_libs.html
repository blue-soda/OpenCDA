<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>opencda.core.sensing.perception.o3d_lidar_libs API documentation</title>
<meta name="description" content="Utility functions for 3d lidar visualization and processing by utilizing open3d." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>opencda.core.sensing.perception.o3d_lidar_libs</code></h1>
</header>
<section id="section-intro">
<p>Utility functions for 3d lidar visualization and processing by utilizing open3d.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
Utility functions for 3d lidar visualization and processing by utilizing open3d.
&#34;&#34;&#34;

# Author: CARLA Team, Runsheng Xu &lt;rxx3386@ucla.edu&gt;
# License: MIT

import time

import open3d as o3d
import numpy as np

from matplotlib import cm
from scipy.stats import mode

import opencda.core.sensing.perception.sensor_transformation as st
from opencda.core.sensing.perception.obstacle_vehicle import is_vehicle_cococlass, ObstacleVehicle, StaticObstacle

VIRIDIS = np.array(cm.get_cmap(&#39;plasma&#39;).colors)
VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])
LABEL_COLORS = np.array([
    (255, 255, 255),  # None
    (70, 70, 70),  # Building
    (100, 40, 40),  # Fences
    (55, 90, 80),  # Other
    (220, 20, 60),  # Pedestrian
    (153, 153, 153),  # Pole
    (157, 234, 50),  # RoadLines
    (128, 64, 128),  # Road
    (244, 35, 232),  # Sidewalk
    (107, 142, 35),  # Vegetation
    (0, 0, 142),  # Vehicle
    (102, 102, 156),  # Wall
    (220, 220, 0),  # TrafficSign
    (70, 130, 180),  # Sky
    (81, 0, 81),  # Ground
    (150, 100, 100),  # Bridge
    (230, 150, 140),  # RailTrack
    (180, 165, 180),  # GuardRail
    (250, 170, 30),  # TrafficLight
    (110, 190, 160),  # Static
    (170, 120, 50),  # Dynamic
    (45, 60, 150),  # Water
    (145, 170, 100),  # Terrain
]) / 255.0  # normalize each channel [0-1] since is what Open3D uses


def o3d_pointcloud_encode(raw_data, point_cloud):
    &#34;&#34;&#34;
    Encode the raw point cloud to Open3d PointCloud object.
    Args:
        raw_data (np.ndarray): Raw lidar points (N, (x, y, z, i)) obtained from lidar sensor.
        point_cloud (o3d.PointCloud):  Open3d PointCloud.

    Returns:
        (o3d.PointCloud): PointCloud with added points.
    &#34;&#34;&#34;

    # Isolate the intensity and compute a color for it
    intensity = raw_data[:, -1]
    intensity_col = 1.0 - np.log(intensity) / np.log(np.exp(-0.004 * 100))
    int_color = np.c_[
        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 0]),
        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 1]),
        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 2])]

    # Isolate the 3D data
    points = raw_data[:, :-1]
    # We&#39;re negating the y to correclty visualize a world that matches
    # what we see in Unreal since Open3D uses a right-handed coordinate system
    points[:, :1] = -points[:, :1]

    point_cloud.points = o3d.utility.Vector3dVector(points)
    point_cloud.colors = o3d.utility.Vector3dVector(int_color)


def o3d_visualizer_init(actor_id):
    &#34;&#34;&#34;
    Initialize the visualizer.
    Args:
        actor_id (int): Vehicle&#39;s id.
    Returns:
        (o3d.visualizer): Initialize Open3d visualizer.

    &#34;&#34;&#34;
    vis = o3d.visualization.Visualizer()
    vis.create_window(window_name=str(actor_id),
                      width=720,
                      height=405,
                      left=480,
                      top=270)
    vis.get_render_option().background_color = [0.05, 0.05, 0.05]
    vis.get_render_option().point_size = 1
    vis.get_render_option().show_coordinate_frame = True

    return vis


def o3d_visualizer_show(vis, count, point_cloud, objects):
    &#34;&#34;&#34;
    Visualize the point cloud at runtime.
    Args:
        vis (o3d.Visualizer): Visualization interface.
        count (int): current step since simulation started.
        point_cloud (o3d.PointCLoud): Open3d point clouds.
        objects (dict): The dictionary containing objects.
    Returns:

    &#34;&#34;&#34;

    if count == 2:
        vis.add_geometry(point_cloud)

    vis.update_geometry(point_cloud)

    for key, object_list in objects.items():
        # we only draw vehicles for now
        if key != &#39;vehicles&#39;:
            continue
        for object_ in object_list:
            aabb = object_.o3d_bbx
            vis.add_geometry(aabb)

    vis.poll_events()
    vis.update_renderer()
    # # This can fix Open3D jittering issues:
    time.sleep(0.001)

    for key, object_list in objects.items():
        if key != &#39;vehicles&#39;:
            continue
        for object_ in object_list:
            aabb = object_.o3d_bbx
            vis.remove_geometry(aabb)


def o3d_camera_lidar_fusion(objects, yolo_bbx, lidar_3d, projected_lidar, lidar_sensor):
    &#34;&#34;&#34;
    Utilize the 3D lidar points to extend the 2D bounding box from camera to 3D bounding box under world coordinates.
    Args:
        objects (dict): The dictionary contains all object detection result.
        yolo_bbx (torch.Tensor): Object detection bounding box at current photo from yolov5,
                                 shape:(n, [x1, y1, x2, y2, label]).
        lidar_3d (np.ndarray): Raw 3D lidar points in lidar coordinate system.
        projected_lidar (np.ndarray): 3D lidar points projected to the camera space.
        lidar_sensor (carla.Sensor): The lidar sensor.

    Returns:
        objects: dict
            The update object dictionary that contains 3d bounding boxes.
    &#34;&#34;&#34;
    
    # convert torch tensor to numpy array first
    if yolo_bbx.is_cuda:
        yolo_bbx = yolo_bbx.cpu().detach().numpy()
    else:
        yolo_bbx = yolo_bbx.detach().numpy()

    for i in range(yolo_bbx.shape[0]):
        detection = yolo_bbx[i]
        # 2d bbx coordinates
        x1, y1, x2, y2 = int(detection[0]), int(detection[1]), int(detection[2]), int(detection[3])
        label = int(detection[5])

        # choose the lidar points in the 2d yolo bounding box
        points_in_bbx = \
            (projected_lidar[:, 0] &gt; x1) &amp; (projected_lidar[:, 0] &lt; x2) &amp; \
            (projected_lidar[:, 1] &gt; y1) &amp; (projected_lidar[:, 1] &lt; y2) &amp; \
            (projected_lidar[:, 2] &gt; 0.0)
        # ignore intensity channel
        select_points = lidar_3d[points_in_bbx][:, :-1]

        if select_points.shape[0] == 0:
            continue

        # filter out the outlier
        x_common = mode(np.array(np.abs(select_points[:, 0]), dtype=np.int), axis=0)[0][0]
        y_common = mode(np.array(np.abs(select_points[:, 1]), dtype=np.int), axis=0)[0][0]
        points_inlier = (np.abs(select_points[:, 0]) &gt; x_common - 3) &amp; (np.abs(select_points[:, 0]) &lt; x_common + 3) &amp; \
                        (np.abs(select_points[:, 1]) &gt; y_common - 3) &amp; (np.abs(select_points[:, 1]) &lt; y_common + 3)
        select_points = select_points[points_inlier]

        if select_points.shape[0] &lt; 2:
            continue

        # to visualize 3d lidar points in o3d visualizer, we need to revert the x coordinates
        select_points[:, :1] = -select_points[:, :1]

        # create o3d.PointCloud object
        o3d_pointcloud = o3d.geometry.PointCloud()
        o3d_pointcloud.points = o3d.utility.Vector3dVector(select_points)
        # add o3d bounding box
        aabb = o3d_pointcloud.get_axis_aligned_bounding_box()
        aabb.color = (0, 1, 0)

        # get the eight corner of the bounding boxes.
        corner = np.asarray(aabb.get_box_points())
        # covert back to unreal coordinate
        corner[:, :1] = -corner[:, :1]
        corner = corner.transpose()
        # extend (3, 8) to (4, 8) for homogenous transformation
        corner = np.r_[corner, [np.ones(corner.shape[1])]]
        # project to world reference
        corner = st.sensor_to_world(corner, lidar_sensor.get_transform())
        corner = corner.transpose()[:, :3]

        if is_vehicle_cococlass(label):
            obstacle_vehicle = ObstacleVehicle(corner, aabb)
            if &#39;vehicles&#39; in objects:
                objects[&#39;vehicles&#39;].append(obstacle_vehicle)
            else:
                objects[&#39;vehicles&#39;] = [obstacle_vehicle]
        # we regard or other obstacle rather than vehicle as static class
        else:
            static_obstacle = StaticObstacle(corner, aabb)
            if &#39;static&#39; in objects:
                objects[&#39;static&#39;].append(static_obstacle)
            else:
                objects[&#39;static&#39;] = [static_obstacle]

    return objects</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="opencda.core.sensing.perception.o3d_lidar_libs.o3d_camera_lidar_fusion"><code class="name flex">
<span>def <span class="ident">o3d_camera_lidar_fusion</span></span>(<span>objects, yolo_bbx, lidar_3d, projected_lidar, lidar_sensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Utilize the 3D lidar points to extend the 2D bounding box from camera to 3D bounding box under world coordinates.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>objects</code></strong> :&ensp;<code>dict</code></dt>
<dd>The dictionary contains all object detection result.</dd>
<dt><strong><code>yolo_bbx</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Object detection bounding box at current photo from yolov5,
shape:(n, [x1, y1, x2, y2, label]).</dd>
<dt><strong><code>lidar_3d</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Raw 3D lidar points in lidar coordinate system.</dd>
<dt><strong><code>projected_lidar</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>3D lidar points projected to the camera space.</dd>
<dt><strong><code>lidar_sensor</code></strong> :&ensp;<code>carla.Sensor</code></dt>
<dd>The lidar sensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>objects</code></dt>
<dd>dict
The update object dictionary that contains 3d bounding boxes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def o3d_camera_lidar_fusion(objects, yolo_bbx, lidar_3d, projected_lidar, lidar_sensor):
    &#34;&#34;&#34;
    Utilize the 3D lidar points to extend the 2D bounding box from camera to 3D bounding box under world coordinates.
    Args:
        objects (dict): The dictionary contains all object detection result.
        yolo_bbx (torch.Tensor): Object detection bounding box at current photo from yolov5,
                                 shape:(n, [x1, y1, x2, y2, label]).
        lidar_3d (np.ndarray): Raw 3D lidar points in lidar coordinate system.
        projected_lidar (np.ndarray): 3D lidar points projected to the camera space.
        lidar_sensor (carla.Sensor): The lidar sensor.

    Returns:
        objects: dict
            The update object dictionary that contains 3d bounding boxes.
    &#34;&#34;&#34;
    
    # convert torch tensor to numpy array first
    if yolo_bbx.is_cuda:
        yolo_bbx = yolo_bbx.cpu().detach().numpy()
    else:
        yolo_bbx = yolo_bbx.detach().numpy()

    for i in range(yolo_bbx.shape[0]):
        detection = yolo_bbx[i]
        # 2d bbx coordinates
        x1, y1, x2, y2 = int(detection[0]), int(detection[1]), int(detection[2]), int(detection[3])
        label = int(detection[5])

        # choose the lidar points in the 2d yolo bounding box
        points_in_bbx = \
            (projected_lidar[:, 0] &gt; x1) &amp; (projected_lidar[:, 0] &lt; x2) &amp; \
            (projected_lidar[:, 1] &gt; y1) &amp; (projected_lidar[:, 1] &lt; y2) &amp; \
            (projected_lidar[:, 2] &gt; 0.0)
        # ignore intensity channel
        select_points = lidar_3d[points_in_bbx][:, :-1]

        if select_points.shape[0] == 0:
            continue

        # filter out the outlier
        x_common = mode(np.array(np.abs(select_points[:, 0]), dtype=np.int), axis=0)[0][0]
        y_common = mode(np.array(np.abs(select_points[:, 1]), dtype=np.int), axis=0)[0][0]
        points_inlier = (np.abs(select_points[:, 0]) &gt; x_common - 3) &amp; (np.abs(select_points[:, 0]) &lt; x_common + 3) &amp; \
                        (np.abs(select_points[:, 1]) &gt; y_common - 3) &amp; (np.abs(select_points[:, 1]) &lt; y_common + 3)
        select_points = select_points[points_inlier]

        if select_points.shape[0] &lt; 2:
            continue

        # to visualize 3d lidar points in o3d visualizer, we need to revert the x coordinates
        select_points[:, :1] = -select_points[:, :1]

        # create o3d.PointCloud object
        o3d_pointcloud = o3d.geometry.PointCloud()
        o3d_pointcloud.points = o3d.utility.Vector3dVector(select_points)
        # add o3d bounding box
        aabb = o3d_pointcloud.get_axis_aligned_bounding_box()
        aabb.color = (0, 1, 0)

        # get the eight corner of the bounding boxes.
        corner = np.asarray(aabb.get_box_points())
        # covert back to unreal coordinate
        corner[:, :1] = -corner[:, :1]
        corner = corner.transpose()
        # extend (3, 8) to (4, 8) for homogenous transformation
        corner = np.r_[corner, [np.ones(corner.shape[1])]]
        # project to world reference
        corner = st.sensor_to_world(corner, lidar_sensor.get_transform())
        corner = corner.transpose()[:, :3]

        if is_vehicle_cococlass(label):
            obstacle_vehicle = ObstacleVehicle(corner, aabb)
            if &#39;vehicles&#39; in objects:
                objects[&#39;vehicles&#39;].append(obstacle_vehicle)
            else:
                objects[&#39;vehicles&#39;] = [obstacle_vehicle]
        # we regard or other obstacle rather than vehicle as static class
        else:
            static_obstacle = StaticObstacle(corner, aabb)
            if &#39;static&#39; in objects:
                objects[&#39;static&#39;].append(static_obstacle)
            else:
                objects[&#39;static&#39;] = [static_obstacle]

    return objects</code></pre>
</details>
</dd>
<dt id="opencda.core.sensing.perception.o3d_lidar_libs.o3d_pointcloud_encode"><code class="name flex">
<span>def <span class="ident">o3d_pointcloud_encode</span></span>(<span>raw_data, point_cloud)</span>
</code></dt>
<dd>
<div class="desc"><p>Encode the raw point cloud to Open3d PointCloud object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>raw_data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Raw lidar points (N, (x, y, z, i)) obtained from lidar sensor.</dd>
<dt><strong><code>point_cloud</code></strong> :&ensp;<code>o3d.PointCloud</code></dt>
<dd>Open3d PointCloud.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(o3d.PointCloud): PointCloud with added points.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def o3d_pointcloud_encode(raw_data, point_cloud):
    &#34;&#34;&#34;
    Encode the raw point cloud to Open3d PointCloud object.
    Args:
        raw_data (np.ndarray): Raw lidar points (N, (x, y, z, i)) obtained from lidar sensor.
        point_cloud (o3d.PointCloud):  Open3d PointCloud.

    Returns:
        (o3d.PointCloud): PointCloud with added points.
    &#34;&#34;&#34;

    # Isolate the intensity and compute a color for it
    intensity = raw_data[:, -1]
    intensity_col = 1.0 - np.log(intensity) / np.log(np.exp(-0.004 * 100))
    int_color = np.c_[
        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 0]),
        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 1]),
        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 2])]

    # Isolate the 3D data
    points = raw_data[:, :-1]
    # We&#39;re negating the y to correclty visualize a world that matches
    # what we see in Unreal since Open3D uses a right-handed coordinate system
    points[:, :1] = -points[:, :1]

    point_cloud.points = o3d.utility.Vector3dVector(points)
    point_cloud.colors = o3d.utility.Vector3dVector(int_color)</code></pre>
</details>
</dd>
<dt id="opencda.core.sensing.perception.o3d_lidar_libs.o3d_visualizer_init"><code class="name flex">
<span>def <span class="ident">o3d_visualizer_init</span></span>(<span>actor_id)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the visualizer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>actor_id</code></strong> :&ensp;<code>int</code></dt>
<dd>Vehicle's id.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(o3d.visualizer): Initialize Open3d visualizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def o3d_visualizer_init(actor_id):
    &#34;&#34;&#34;
    Initialize the visualizer.
    Args:
        actor_id (int): Vehicle&#39;s id.
    Returns:
        (o3d.visualizer): Initialize Open3d visualizer.

    &#34;&#34;&#34;
    vis = o3d.visualization.Visualizer()
    vis.create_window(window_name=str(actor_id),
                      width=720,
                      height=405,
                      left=480,
                      top=270)
    vis.get_render_option().background_color = [0.05, 0.05, 0.05]
    vis.get_render_option().point_size = 1
    vis.get_render_option().show_coordinate_frame = True

    return vis</code></pre>
</details>
</dd>
<dt id="opencda.core.sensing.perception.o3d_lidar_libs.o3d_visualizer_show"><code class="name flex">
<span>def <span class="ident">o3d_visualizer_show</span></span>(<span>vis, count, point_cloud, objects)</span>
</code></dt>
<dd>
<div class="desc"><p>Visualize the point cloud at runtime.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vis</code></strong> :&ensp;<code>o3d.Visualizer</code></dt>
<dd>Visualization interface.</dd>
<dt><strong><code>count</code></strong> :&ensp;<code>int</code></dt>
<dd>current step since simulation started.</dd>
<dt><strong><code>point_cloud</code></strong> :&ensp;<code>o3d.PointCLoud</code></dt>
<dd>Open3d point clouds.</dd>
<dt><strong><code>objects</code></strong> :&ensp;<code>dict</code></dt>
<dd>The dictionary containing objects.</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def o3d_visualizer_show(vis, count, point_cloud, objects):
    &#34;&#34;&#34;
    Visualize the point cloud at runtime.
    Args:
        vis (o3d.Visualizer): Visualization interface.
        count (int): current step since simulation started.
        point_cloud (o3d.PointCLoud): Open3d point clouds.
        objects (dict): The dictionary containing objects.
    Returns:

    &#34;&#34;&#34;

    if count == 2:
        vis.add_geometry(point_cloud)

    vis.update_geometry(point_cloud)

    for key, object_list in objects.items():
        # we only draw vehicles for now
        if key != &#39;vehicles&#39;:
            continue
        for object_ in object_list:
            aabb = object_.o3d_bbx
            vis.add_geometry(aabb)

    vis.poll_events()
    vis.update_renderer()
    # # This can fix Open3D jittering issues:
    time.sleep(0.001)

    for key, object_list in objects.items():
        if key != &#39;vehicles&#39;:
            continue
        for object_ in object_list:
            aabb = object_.o3d_bbx
            vis.remove_geometry(aabb)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="opencda.core.sensing.perception" href="index.html">opencda.core.sensing.perception</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="opencda.core.sensing.perception.o3d_lidar_libs.o3d_camera_lidar_fusion" href="#opencda.core.sensing.perception.o3d_lidar_libs.o3d_camera_lidar_fusion">o3d_camera_lidar_fusion</a></code></li>
<li><code><a title="opencda.core.sensing.perception.o3d_lidar_libs.o3d_pointcloud_encode" href="#opencda.core.sensing.perception.o3d_lidar_libs.o3d_pointcloud_encode">o3d_pointcloud_encode</a></code></li>
<li><code><a title="opencda.core.sensing.perception.o3d_lidar_libs.o3d_visualizer_init" href="#opencda.core.sensing.perception.o3d_lidar_libs.o3d_visualizer_init">o3d_visualizer_init</a></code></li>
<li><code><a title="opencda.core.sensing.perception.o3d_lidar_libs.o3d_visualizer_show" href="#opencda.core.sensing.perception.o3d_lidar_libs.o3d_visualizer_show">o3d_visualizer_show</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>